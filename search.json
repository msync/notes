[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Study notes for quick access from anywhere."
  },
  {
    "objectID": "nn/index.html",
    "href": "nn/index.html",
    "title": "Building Blocks of the Deep Learning Model",
    "section": "",
    "text": "It’s signal in, and signal out. The transformation function is the key - it can literally be anything. Intuitively, modeling based on our understanding of the neuron, a simpler function is a good choice. But we do not choose linear functions as they can not model complex relationships.\nStacking multiple of them leads to obviously interesting complex functions. But we do not know the parameters of this large function that the entire network represents. We need to learn them.\n\n\nCode\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom micrograd.engine import Value\nfrom micrograd.nn import Neuron\nfrom sklearn.datasets import make_moons\n\nnp.random.seed(1337)\nrandom.seed(1337)\n\n\n\nX, y = make_moons(n_samples=100, noise=0.1)\n\ny = y*2 - 1\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n\n\n\n\n\n\n\n\nThe dataset is a simple 2D dataset with two classes. We can see that the classes are not linearly separable.\nWhat we want is a function that, given any of those points, can tell us which class it belongs to. The simplest way to do it is to define a dictionary that maps each point to a class. But that’s not what we want, because that’s not a realistic problem to solve.\n\n\nThe promise of the neural network is, like the brain, to generalize. That is, to be able to predict the class of a point that it has never seen before. This is the key to the success of neural networks.\nSo, we imagine that the solution is a function that can, given any point in this 2D space, choose a “side.” This side is the class that the point belongs to. So, a curve that can somehoe separate the two classes is what we are looking for.\nAs we have stated before (without any formal proof), a network of stacked perceptrons can model any function. So, let’s model this conjectured function with a network of perceptrons."
  },
  {
    "objectID": "nn/index.html#generalization",
    "href": "nn/index.html#generalization",
    "title": "Building Blocks of the Deep Learning Model",
    "section": "",
    "text": "The promise of the neural network is, like the brain, to generalize. That is, to be able to predict the class of a point that it has never seen before. This is the key to the success of neural networks.\nSo, we imagine that the solution is a function that can, given any point in this 2D space, choose a “side.” This side is the class that the point belongs to. So, a curve that can somehoe separate the two classes is what we are looking for.\nAs we have stated before (without any formal proof), a network of stacked perceptrons can model any function. So, let’s model this conjectured function with a network of perceptrons."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sync’ing Memory to Notes",
    "section": "",
    "text": "Created using Quarto. Check out https://quarto.org/docs/websites."
  }
]